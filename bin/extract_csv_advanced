#!/usr/bin/env ruby
# frozen_string_literal: true

require_relative "../lib/ic_metrics"
require "csv"
require "date"

# Advanced CSV Extractor with text analysis features
class AdvancedCsvExtractor
  def initialize
    @config = IcMetrics::Config.new
    @data_dir = @config.data_directory
  end

  def run(args)
    if args.empty?
      show_help
      exit 1
    end

    command = args[0]
    
    case command
    when "extract"
      extract_command(args[1..-1])
    when "analyze"
      analyze_command(args[1..-1])
    when "merge"
      merge_command(args[1..-1])
    else
      puts "Unknown command: #{command}"
      show_help
      exit 1
    end
  end

  private

  def show_help
    puts "Advanced CSV Extractor for IC Metrics"
    puts ""
    puts "USAGE:"
    puts "  ruby bin/extract_csv_advanced <command> [options]"
    puts ""
    puts "COMMANDS:"
    puts "  extract <username> [output_dir]  Extract all user content to CSV files"
    puts "  analyze <username>               Generate content analysis report"
    puts "  merge <username>                 Create a single merged CSV with all content"
    puts ""
    puts "Available users:"
    list_available_users
  end

  def list_available_users
    return puts "  No data found in #{@data_dir}" unless Dir.exist?(@data_dir)

    Dir.glob(File.join(@data_dir, "*")).each do |user_dir|
      next unless File.directory?(user_dir)
      
      username = File.basename(user_dir)
      contributions_file = File.join(user_dir, "contributions.json")
      
      if File.exist?(contributions_file)
        puts "  #{username}"
      end
    end
  end

  def extract_command(args)
    username = args[0]
    output_dir = args[1] || File.join(@data_dir, username, "csv_exports")

    unless username
      puts "Error: Username required"
      puts "Usage: ruby bin/extract_csv_advanced extract <username> [output_dir]"
      exit 1
    end

    extract_enhanced_csv(username, output_dir)
  end

  def analyze_command(args)
    username = args[0]
    
    unless username
      puts "Error: Username required"
      puts "Usage: ruby bin/extract_csv_advanced analyze <username>"
      exit 1
    end

    generate_content_analysis(username)
  end

  def merge_command(args)
    username = args[0]
    output_file = args[1] || File.join(@data_dir, username, "all_contributions.csv")

    unless username
      puts "Error: Username required"
      puts "Usage: ruby bin/extract_csv_advanced merge <username> [output_file]"
      exit 1
    end

    create_merged_csv(username, output_file)
  end

  def extract_enhanced_csv(username, output_dir)
    contributions_file = File.join(@data_dir, username, "contributions.json")
    
    unless File.exist?(contributions_file)
      puts "Error: No contribution data found for #{username}"
      exit 1
    end

    puts "Loading contribution data for #{username}..."
    data = JSON.parse(File.read(contributions_file))

    FileUtils.mkdir_p(output_dir)
    puts "Extracting enhanced CSV to: #{output_dir}"

    # Extract with enhanced analysis
    extract_commits_enhanced(data, output_dir)
    extract_text_content_analysis(data, output_dir)
    extract_timeline_csv(data, output_dir)
    
    puts "Enhanced CSV extraction completed!"
  end

  def extract_commits_enhanced(data, output_dir)
    csv_file = File.join(output_dir, "commits_enhanced.csv")
    
    CSV.open(csv_file, "w") do |csv|
      csv << [
        "repository", "sha", "message", "message_length", "message_type",
        "author_date", "day_of_week", "hour", "month",
        "additions", "deletions", "total_changes", "files_changed",
        "conventional_commit", "url"
      ]

      data["repositories"].each do |repo_name, repo_data|
        repo_data["commits"].each do |commit|
          commit_data = commit["commit"]
          message = commit_data["message"]&.strip || ""
          author_date = Time.parse(commit_data["author"]["date"])
          
          # Analyze commit message
          is_conventional = conventional_commit?(message)
          message_type = extract_commit_type(message)
          
          csv << [
            repo_name,
            commit["sha"],
            message.gsub(/\n/, " "),
            message.length,
            message_type,
            commit_data["author"]["date"],
            author_date.strftime("%A"),
            author_date.hour,
            author_date.strftime("%B"),
            commit["stats"]&.dig("additions") || 0,
            commit["stats"]&.dig("deletions") || 0,
            commit["stats"]&.dig("total") || 0,
            commit["files"]&.size || 0,
            is_conventional,
            commit["html_url"]
          ]
        end
      end
    end

    puts "  ✓ Enhanced commits exported"
  end

  def extract_text_content_analysis(data, output_dir)
    csv_file = File.join(output_dir, "text_content_analysis.csv")
    
    CSV.open(csv_file, "w") do |csv|
      csv << [
        "repository", "content_type", "content_id", "title", "body",
        "body_length", "word_count", "line_count", "has_code_blocks",
        "has_links", "has_mentions", "created_at", "url"
      ]

      data["repositories"].each do |repo_name, repo_data|
        # PRs
        repo_data["pull_requests"].each do |pr|
          body = pr["body"] || ""
          csv << [
            repo_name, "pull_request", pr["number"], pr["title"], body,
            body.length, word_count(body), line_count(body),
            has_code_blocks?(body), has_links?(body), has_mentions?(body),
            pr["created_at"], pr["html_url"]
          ]
        end

        # Issues
        repo_data["issues"].each do |issue|
          next if issue["pull_request"] # Skip PRs that appear as issues
          
          body = issue["body"] || ""
          csv << [
            repo_name, "issue", issue["number"], issue["title"], body,
            body.length, word_count(body), line_count(body),
            has_code_blocks?(body), has_links?(body), has_mentions?(body),
            issue["created_at"], issue["html_url"]
          ]
        end

        # Comments
        [
          [repo_data["pr_comments"] || [], "pr_comment"],
          [repo_data["issue_comments"] || [], "issue_comment"],
          [repo_data["reviews"] || [], "review"]
        ].each do |comments, type|
          comments.each do |comment|
            body = comment["body"] || ""
            next if body.empty?
            
            csv << [
              repo_name, type, comment["id"], "", body,
              body.length, word_count(body), line_count(body),
              has_code_blocks?(body), has_links?(body), has_mentions?(body),
              comment["created_at"] || comment["submitted_at"], comment["html_url"]
            ]
          end
        end
      end
    end

    puts "  ✓ Text content analysis exported"
  end

  def extract_timeline_csv(data, output_dir)
    csv_file = File.join(output_dir, "activity_timeline.csv")
    
    activities = []

    data["repositories"].each do |repo_name, repo_data|
      # Commits
      repo_data["commits"].each do |commit|
        activities << {
          repository: repo_name,
          type: "commit",
          id: commit["sha"],
          title: commit["commit"]["message"]&.split("\n")&.first || "",
          date: commit["commit"]["author"]["date"],
          url: commit["html_url"]
        }
      end

      # PRs
      repo_data["pull_requests"].each do |pr|
        activities << {
          repository: repo_name,
          type: "pull_request",
          id: pr["number"],
          title: pr["title"],
          date: pr["created_at"],
          url: pr["html_url"]
        }
      end

      # Reviews
      repo_data["reviews"].each do |review|
        activities << {
          repository: repo_name,
          type: "review",
          id: review["id"],
          title: review["state"],
          date: review["submitted_at"],
          url: review["html_url"]
        }
      end

      # Issues
      repo_data["issues"].each do |issue|
        next if issue["pull_request"]
        
        activities << {
          repository: repo_name,
          type: "issue",
          id: issue["number"],
          title: issue["title"],
          date: issue["created_at"],
          url: issue["html_url"]
        }
      end
    end

    # Sort by date
    activities.sort_by! { |activity| Time.parse(activity[:date]) }

    CSV.open(csv_file, "w") do |csv|
      csv << ["date", "repository", "type", "id", "title", "url"]
      
      activities.each do |activity|
        csv << [
          activity[:date],
          activity[:repository],
          activity[:type],
          activity[:id],
          activity[:title],
          activity[:url]
        ]
      end
    end

    puts "  ✓ Activity timeline exported"
  end

  def create_merged_csv(username, output_file)
    contributions_file = File.join(@data_dir, username, "contributions.json")
    
    unless File.exist?(contributions_file)
      puts "Error: No contribution data found for #{username}"
      exit 1
    end

    data = JSON.parse(File.read(contributions_file))
    FileUtils.mkdir_p(File.dirname(output_file))

    CSV.open(output_file, "w") do |csv|
      csv << [
        "repository", "type", "id", "title", "body", "date", 
        "author", "state", "url", "metadata"
      ]

      data["repositories"].each do |repo_name, repo_data|
        # All content types in one CSV
        repo_data["commits"].each do |commit|
          csv << [
            repo_name, "commit", commit["sha"], 
            commit["commit"]["message"]&.split("\n")&.first, 
            commit["commit"]["message"], commit["commit"]["author"]["date"],
            commit["commit"]["author"]["name"], "committed", commit["html_url"],
            "changes:#{commit["stats"]&.dig("total") || 0}"
          ]
        end

        repo_data["pull_requests"].each do |pr|
          csv << [
            repo_name, "pull_request", pr["number"], pr["title"], pr["body"],
            pr["created_at"], pr["user"]["login"], pr["state"], pr["html_url"],
            "additions:#{pr["additions"]};deletions:#{pr["deletions"]}"
          ]
        end

        repo_data["issues"].each do |issue|
          next if issue["pull_request"]
          csv << [
            repo_name, "issue", issue["number"], issue["title"], issue["body"],
            issue["created_at"], issue["user"]["login"], issue["state"], issue["html_url"],
            "comments:#{issue["comments"]}"
          ]
        end
      end
    end

    puts "Merged CSV created: #{output_file}"
  end

  def generate_content_analysis(username)
    contributions_file = File.join(@data_dir, username, "contributions.json")
    
    unless File.exist?(contributions_file)
      puts "Error: No contribution data found for #{username}"
      exit 1
    end

    data = JSON.parse(File.read(contributions_file))
    
    puts "Content Analysis for #{username}"
    puts "=" * 50
    
    # Analyze text content
    all_text = []
    commit_messages = []
    
    data["repositories"].each do |repo_name, repo_data|
      repo_data["commits"].each { |c| commit_messages << c["commit"]["message"] }
      repo_data["pull_requests"].each { |pr| all_text << pr["body"] if pr["body"] }
      repo_data["issues"].each { |issue| all_text << issue["body"] if issue["body"] }
    end

    puts "Writing Patterns:"
    puts "  Total text entries: #{all_text.size}"
    puts "  Average text length: #{all_text.map(&:length).sum / all_text.size.to_f rescue 0} characters"
    puts "  Commit messages: #{commit_messages.size}"
    puts "  Conventional commits: #{commit_messages.count { |msg| conventional_commit?(msg) }}"
    puts ""

    puts "Activity Distribution:"
    data["repositories"].each do |repo, repo_data|
      total = repo_data["commits"].size + repo_data["pull_requests"].size + repo_data["issues"].size
      puts "  #{repo}: #{total} total activities"
    end
  end

  # Helper methods
  def conventional_commit?(message)
    message.match?(/^(feat|fix|docs|style|refactor|test|chore|perf|ci|build|revert)(\(.+\))?:/)
  end

  def extract_commit_type(message)
    return "conventional" if conventional_commit?(message)
    return "merge" if message.downcase.include?("merge")
    return "fix" if message.downcase.match?(/fix|bug/)
    return "feature" if message.downcase.match?(/feat|add|new/)
    "other"
  end

  def word_count(text)
    text&.split&.size || 0
  end

  def line_count(text)
    text&.lines&.size || 0
  end

  def has_code_blocks?(text)
    text&.include?("```") || text&.include?("`") || false
  end

  def has_links?(text)
    text&.match?(/https?:\/\//) || false
  end

  def has_mentions?(text)
    text&.include?("@") || false
  end
end

# Run the advanced extractor
if __FILE__ == $0
  extractor = AdvancedCsvExtractor.new
  extractor.run(ARGV)
end
